  0%|                                                                                                                                     | 0/70 [00:00<?, ?it/s]
{'train_runtime': 1.844, 'train_samples_per_second': 2657.325, 'train_steps_per_second': 37.962, 'train_loss': 0.0, 'epoch': 9.03}
2025-01-09 02:54:05,241 [INFO] Saving LoRA adapter...
2025-01-09 02:54:05,740 [INFO] Merging LoRA weights...
/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 60, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
2025-01-09 02:54:06,300 [INFO] Exporting to ONNX format...
2025-01-09 02:54:06,300 [INFO] Exporting model to ONNX format...
2025-01-09 02:54:06,300 [INFO] Converting model to full precision for ONNX export...
2025-01-09 02:54:06,300 [ERROR] Failed to export model to ONNX: You cannot cast a bitsandbytes model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired `dtype` by passing the correct `torch_dtype` argument.
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/main/train_finbot_qlora.py", line 146, in export_to_onnx
    model = model.to(torch.float32)
            ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/modeling_utils.py", line 2937, in to
    raise ValueError(
ValueError: You cannot cast a bitsandbytes model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired `dtype` by passing the correct `torch_dtype` argument.
2025-01-09 02:54:06,301 [ERROR] ONNX export failed, but training completed successfully: You cannot cast a bitsandbytes model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired `dtype` by passing the correct `torch_dtype` argument.
2025-01-09 02:54:06,301 [INFO] Training completed successfully!
