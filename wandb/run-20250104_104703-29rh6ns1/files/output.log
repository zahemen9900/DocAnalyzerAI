100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 229/229 [23:35<00:00,  6.18s/it]
{'loss': 10.5393, 'grad_norm': 3.2569334506988525, 'learning_rate': 4.347826086956522e-06, 'epoch': 0.09}
{'loss': 10.452, 'grad_norm': 3.0825393199920654, 'learning_rate': 4.9164513914144005e-06, 'epoch': 0.17}
{'loss': 10.2554, 'grad_norm': 3.2592029571533203, 'learning_rate': 4.612452562309975e-06, 'epoch': 0.26}
2025-01-04 11:10:40,675 [INFO] Saving LoRA adapter...                                                                                                        
{'eval_loss': 10.071575164794922, 'eval_runtime': 136.8379, 'eval_samples_per_second': 46.04, 'eval_steps_per_second': 5.759, 'epoch': 0.3}
{'loss': 10.159, 'grad_norm': 3.5985138416290283, 'learning_rate': 4.1134500737541026e-06, 'epoch': 0.35}
{'loss': 9.9229, 'grad_norm': 3.856266975402832, 'learning_rate': 3.4655075927279576e-06, 'epoch': 0.44}
{'loss': 9.821, 'grad_norm': 3.6244661808013916, 'learning_rate': 2.7284376599571776e-06, 'epoch': 0.52}
{'loss': 9.8325, 'grad_norm': 4.17053747177124, 'learning_rate': 1.970280304707447e-06, 'epoch': 0.61}
{'eval_loss': 9.487751960754395, 'eval_runtime': 137.8551, 'eval_samples_per_second': 45.7, 'eval_steps_per_second': 5.716, 'epoch': 0.61}
{'loss': 9.5799, 'grad_norm': 4.152774333953857, 'learning_rate': 1.261022167792161e-06, 'epoch': 0.7}
{'loss': 9.6497, 'grad_norm': 4.545602321624756, 'learning_rate': 6.661359299530626e-07, 'epoch': 0.78}
{'loss': 9.4323, 'grad_norm': 4.383073806762695, 'learning_rate': 2.405364306547955e-07, 'epoch': 0.87}
{'eval_loss': 9.301474571228027, 'eval_runtime': 136.4671, 'eval_samples_per_second': 46.165, 'eval_steps_per_second': 5.774, 'epoch': 0.91}
{'loss': 9.54, 'grad_norm': 4.28942346572876, 'learning_rate': 2.351139701825267e-08, 'epoch': 0.96}
{'train_runtime': 1419.2688, 'train_samples_per_second': 10.357, 'train_steps_per_second': 0.161, 'train_loss': 9.913803167218203, 'epoch': 1.0}
2025-01-04 11:10:41,208 [INFO] Merging LoRA weights...
/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 60, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
2025-01-04 11:10:41,783 [INFO] Training completed successfully!
