100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 229/229 [23:38<00:00,  6.20s/it]
{'loss': 10.2246, 'grad_norm': 2.7795534133911133, 'learning_rate': 4.347826086956522e-06, 'epoch': 0.09}
{'loss': 10.2194, 'grad_norm': 2.9108574390411377, 'learning_rate': 4.9164513914144005e-06, 'epoch': 0.17}
{'loss': 10.0203, 'grad_norm': 3.154442071914673, 'learning_rate': 4.612452562309975e-06, 'epoch': 0.26}
2025-01-03 01:13:32,940 [INFO] Saving LoRA adapter...                                                                                                                          
{'eval_loss': 9.912936210632324, 'eval_runtime': 137.776, 'eval_samples_per_second': 45.726, 'eval_steps_per_second': 5.719, 'epoch': 0.3}
{'loss': 9.911, 'grad_norm': 3.205629348754883, 'learning_rate': 4.1134500737541026e-06, 'epoch': 0.35}
{'loss': 9.7626, 'grad_norm': 3.5780892372131348, 'learning_rate': 3.4655075927279576e-06, 'epoch': 0.44}
{'loss': 9.609, 'grad_norm': 3.538611650466919, 'learning_rate': 2.7284376599571776e-06, 'epoch': 0.52}
{'loss': 9.567, 'grad_norm': 4.13842248916626, 'learning_rate': 1.970280304707447e-06, 'epoch': 0.61}
{'eval_loss': 9.350053787231445, 'eval_runtime': 137.8558, 'eval_samples_per_second': 45.7, 'eval_steps_per_second': 5.716, 'epoch': 0.61}
{'loss': 9.405, 'grad_norm': 4.152933597564697, 'learning_rate': 1.261022167792161e-06, 'epoch': 0.7}
{'loss': 9.3966, 'grad_norm': 4.464153289794922, 'learning_rate': 6.661359299530626e-07, 'epoch': 0.78}
{'loss': 9.34, 'grad_norm': 4.005578994750977, 'learning_rate': 2.405364306547955e-07, 'epoch': 0.87}
{'eval_loss': 9.169624328613281, 'eval_runtime': 136.6243, 'eval_samples_per_second': 46.112, 'eval_steps_per_second': 5.768, 'epoch': 0.91}
{'loss': 9.354, 'grad_norm': 4.187119960784912, 'learning_rate': 2.351139701825267e-08, 'epoch': 0.96}
{'train_runtime': 1420.988, 'train_samples_per_second': 10.345, 'train_steps_per_second': 0.161, 'train_loss': 9.693385361583992, 'epoch': 1.0}
2025-01-03 01:13:33,522 [INFO] Merging LoRA weights...
/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 60, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
2025-01-03 01:13:34,128 [INFO] Training completed successfully!
