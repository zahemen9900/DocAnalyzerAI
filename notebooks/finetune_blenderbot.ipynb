{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98be044eb4df42aba664937ef94b3e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    }
   ],
   "source": [
    "#Login into my huggingface account in notebook\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add this at the start of your script\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Configure logging\u001b[39;00m\n\u001b[1;32m     26\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(\n\u001b[1;32m     27\u001b[0m     level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     29\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/cuda/memory.py:192\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BlenderbotTokenizer, \n",
    "    BlenderbotForConditionalGeneration,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig \n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from huggingface_hub import notebook_login, HfApi\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import bitsandbytes as bnb\n",
    "from datetime import datetime\n",
    "torch.cuda.empty_cache()  # Add this at the start of your script\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset(data_dir: str):\n",
    "    \"\"\"Load and prepare the dataset for training\"\"\"\n",
    "    # Load the processed datasets\n",
    "    dataset = load_dataset(\n",
    "        'json', \n",
    "        data_files={\n",
    "            'train': f'{data_dir}/train.json',\n",
    "            'validation': f'{data_dir}/val.json'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def prepare_qlora_model(model_name: str):\n",
    "    \"\"\"Prepare model for QLoRA training with better memory handling\"\"\"\n",
    "    try:\n",
    "        # Define quantization config with more conservative settings\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_quant_storage=torch.float32  # More stable storage type\n",
    "        )\n",
    "        \n",
    "        # Load model with gradient checkpointing\n",
    "        model = BlenderbotForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            torch_dtype=torch.float16,\n",
    "            use_cache=False  # Disable KV cache for training\n",
    "        )\n",
    "        \n",
    "        # Manually move model to appropriate device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Enable gradient checkpointing for memory efficiency\n",
    "        model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # Prepare model for training with memory optimizations\n",
    "        model = prepare_model_for_kbit_training(\n",
    "            model,\n",
    "            use_gradient_checkpointing=True\n",
    "        )\n",
    "        \n",
    "        # More conservative LoRA config\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,  # Reduced rank\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"q_proj\", \"k_proj\"],  # Focus on key modules\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        )\n",
    "        \n",
    "        model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in model preparation: {e}\")\n",
    "        raise\n",
    "\n",
    "def setup_tensorboard(output_dir: str):\n",
    "    \"\"\"Setup TensorBoard logging\"\"\"\n",
    "    current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    return SummaryWriter(f'{output_dir}/runs/{current_time}')\n",
    "\n",
    "def train_model(\n",
    "    model_name: str = \"facebook/blenderbot-400M-distill\",\n",
    "    data_dir: str = \"/home/zahemen/projects/dl-lib/DocAnalyzerAI/finetune_data\",\n",
    "    output_dir: str = \"../models/financial_chatbot\",\n",
    "    num_epochs: int = 3,\n",
    "    batch_size: int = 1,  # Reduced batch size\n",
    "    gradient_accumulation_steps: int = 8,  # Increased gradient accumulation\n",
    "    push_to_hub: bool = True,\n",
    "    hub_model_id: str = None\n",
    "):\n",
    "    \"\"\"Fine-tune BlenderBot with memory optimizations\"\"\"\n",
    "    try:\n",
    "        # Clear CUDA cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Initialize tensorboard\n",
    "        writer = setup_tensorboard(output_dir)\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
    "        model = prepare_qlora_model(model_name)\n",
    "        \n",
    "        # Load dataset\n",
    "        dataset = prepare_dataset(data_dir)\n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            # Properly format inputs for Blenderbot\n",
    "            inputs = []\n",
    "            for ctx, msg in zip(examples['additional_context'], examples['free_messages']):\n",
    "                # Combine context and message with proper separator\n",
    "                combined = f\"{ctx} <sep> {msg[0]}\" if isinstance(msg, list) else f\"{ctx} <sep> {msg}\"\n",
    "                inputs.append(combined)\n",
    "            \n",
    "            # Format targets\n",
    "            targets = []\n",
    "            for msg in examples['guided_messages']:\n",
    "                # Handle both list and string inputs\n",
    "                target = msg[0] if isinstance(msg, list) else msg\n",
    "                targets.append(target)\n",
    "            \n",
    "            # Tokenize inputs\n",
    "            model_inputs = tokenizer(\n",
    "                inputs,\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            # Tokenize targets\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                labels = tokenizer(\n",
    "                    targets,\n",
    "                    max_length=512,\n",
    "                    padding='max_length',\n",
    "                    truncation=True\n",
    "                )\n",
    "            \n",
    "            model_inputs['labels'] = labels['input_ids']\n",
    "            \n",
    "            # Ensure all tensors are on the correct device\n",
    "            for k, v in model_inputs.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    model_inputs[k] = v.to(model.device)\n",
    "            \n",
    "            return model_inputs\n",
    "        \n",
    "        # Process datasets with new preprocessing function\n",
    "        processed_datasets = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset['train'].column_names,\n",
    "            desc=\"Processing dataset\"\n",
    "        )\n",
    "        \n",
    "        # Modified training arguments for stability\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=100,  # More frequent evaluation\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=100,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_dir=f\"{output_dir}/logs\",\n",
    "            logging_steps=10,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            push_to_hub=push_to_hub,\n",
    "            hub_model_id=hub_model_id,\n",
    "            # Added stability settings\n",
    "            fp16=True,\n",
    "            optim=\"adamw_torch\",\n",
    "            learning_rate=2e-5,\n",
    "            warmup_ratio=0.1,\n",
    "            max_grad_norm=0.3,\n",
    "            gradient_checkpointing=True\n",
    "        )\n",
    "        \n",
    "        class CustomTrainer(Trainer):\n",
    "            def log(self, logs: dict) -> None:\n",
    "                \"\"\"Custom logging to TensorBoard\"\"\"\n",
    "                super().log(logs)\n",
    "                if self.state.global_step % self.args.logging_steps == 0:\n",
    "                    for key, value in logs.items():\n",
    "                        writer.add_scalar(key, value, self.state.global_step)\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=processed_datasets['train'],\n",
    "            eval_dataset=processed_datasets['validation'],\n",
    "        )\n",
    "        \n",
    "        # Login to HuggingFace if pushing to hub\n",
    "        if push_to_hub:\n",
    "            notebook_login()\n",
    "        \n",
    "        # Train model\n",
    "        logger.info(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save final model and tokenizer\n",
    "        trainer.save_model()\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        # Push to HuggingFace Hub if specified\n",
    "        if push_to_hub and hub_model_id:\n",
    "            logger.info(f\"Pushing model to HuggingFace Hub: {hub_model_id}\")\n",
    "            api = HfApi()\n",
    "            api.upload_folder(\n",
    "                folder_path=output_dir,\n",
    "                repo_id=hub_model_id,\n",
    "                repo_type=\"model\"\n",
    "            )\n",
    "        \n",
    "        # Close tensorboard writer\n",
    "        writer.close()\n",
    "        logger.info(f\"Model saved to {output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training error: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-25 15:06:09,752 - __main__ - INFO - Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "2024-12-25 15:06:09,753 - __main__ - INFO - Available memory: 4.29 GB\n",
      "2024-12-25 15:06:09,755 - __main__ - ERROR - Training error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "2024-12-25 15:06:09,756 - __main__ - ERROR - Training failed: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     25\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_LAUNCH_BLOCKING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Add this line for debugging\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU available, using CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     hub_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzahemen9900/blenderbot-400m-financial\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhub_model_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhub_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Start with batch size 1\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     20\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 121\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_name, data_dir, output_dir, num_epochs, batch_size, gradient_accumulation_steps, push_to_hub, hub_model_id)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# Clear CUDA cache\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 121\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# Initialize tensorboard\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     writer \u001b[38;5;241m=\u001b[39m setup_tensorboard(output_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/cuda/memory.py:192\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Verify CUDA availability\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            logger.info(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        else:\n",
    "            logger.warning(\"No GPU available, using CPU\")\n",
    "        \n",
    "        hub_model_id = \"zahemen9900/blenderbot-400m-financial\"\n",
    "        \n",
    "        train_model(\n",
    "            push_to_hub=True,\n",
    "            hub_model_id=hub_model_id,\n",
    "            num_epochs=5,\n",
    "            batch_size=1,  # Start with batch size 1\n",
    "            gradient_accumulation_steps=8\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Add this line for debugging\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
