2024-12-30 18:55:34,265 [INFO] Using device: cpu (quantization requires CPU)
2024-12-30 18:55:34,265 [INFO] Loading dataset...
2024-12-30 18:55:34,271 [INFO] Loading facebook/blenderbot-400M-distill...
2024-12-30 18:55:35,909 [INFO] Processing datasets...
2024-12-30 18:55:37,844 [INFO] Quantizing model...
2024-12-30 18:55:37,844 [INFO] Starting model quantization...
2024-12-30 18:55:37,974 [INFO] Model quantization completed successfully
2024-12-30 18:55:37,974 [INFO] Starting quantized training...
2024-12-30 18:56:50,303 [INFO] Using device: cpu (quantization requires CPU)
2024-12-30 18:56:50,303 [INFO] Loading dataset...
2024-12-30 18:56:50,311 [INFO] Loading facebook/blenderbot-400M-distill...
2024-12-30 18:56:52,649 [INFO] Processing datasets...
2024-12-30 18:56:55,395 [INFO] Quantizing model...
2024-12-30 18:56:55,395 [INFO] Starting model quantization...
2024-12-30 18:56:55,548 [INFO] Model quantization completed successfully
2024-12-30 18:56:55,548 [INFO] Starting quantized training...
2024-12-30 19:14:57,668 [INFO] Saving quantized model...
2024-12-30 19:14:59,624 [INFO] Training completed and quantized model saved!
2024-12-30 19:27:23,351 [INFO] Using device: cpu (quantization requires CPU)
2024-12-30 19:27:23,351 [INFO] Loading dataset...
2024-12-30 19:27:23,363 [INFO] Loading facebook/blenderbot-400M-distill...
2024-12-30 19:27:25,236 [INFO] Processing datasets...
2024-12-30 19:27:25,675 [INFO] Using default tokenizer.
2024-12-30 19:27:31,583 [INFO] Quantizing model...
2024-12-30 19:27:31,583 [INFO] Starting enhanced quantization process...
2024-12-30 19:27:31,583 [ERROR] Enhanced quantization failed: module 'torch.quantization' has no attribute 'get_default_dynamic_qconfig'
2024-12-30 19:27:31,583 [ERROR] Enhanced training failed: module 'torch.quantization' has no attribute 'get_default_dynamic_qconfig'
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 293, in train_quantized
    quantized_model = trainer.quantize_model()
                      ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 126, in quantize_model
    '': torch.quantization.get_default_dynamic_qconfig(),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'torch.quantization' has no attribute 'get_default_dynamic_qconfig'
2024-12-30 19:27:31,708 [ERROR] Script failed: module 'torch.quantization' has no attribute 'get_default_dynamic_qconfig'
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 329, in <module>
    train_quantized()
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 293, in train_quantized
    quantized_model = trainer.quantize_model()
                      ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 126, in quantize_model
    '': torch.quantization.get_default_dynamic_qconfig(),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'torch.quantization' has no attribute 'get_default_dynamic_qconfig'
2024-12-30 19:32:48,034 [INFO] Using device: cpu
2024-12-30 19:32:48,035 [INFO] Loading dataset...
2024-12-30 19:32:48,048 [INFO] Loading facebook/blenderbot-400M-distill...
2024-12-30 19:32:49,843 [INFO] Processing datasets...
2024-12-30 19:32:50,655 [INFO] Using default tokenizer.
2024-12-30 19:32:57,777 [INFO] Starting quantization process...
2024-12-30 19:32:57,777 [INFO] Starting model quantization...
2024-12-30 19:32:58,195 [INFO] Model quantization completed successfully
2024-12-30 19:32:58,195 [INFO] Quantization successful
2024-12-30 19:32:58,195 [INFO] Starting training...
2024-12-30 20:32:50,579 [INFO] Using device: cpu
2024-12-30 20:32:50,580 [INFO] Loading dataset...
2024-12-30 20:32:50,601 [INFO] Loading facebook/blenderbot-400M-distill...
2024-12-30 20:32:52,472 [INFO] Processing datasets...
2024-12-30 20:32:52,915 [INFO] Using default tokenizer.
2024-12-30 20:34:43,097 [INFO] Starting quantization process...
2024-12-30 20:34:43,097 [INFO] Starting model quantization...
2024-12-30 20:34:43,348 [INFO] Model quantization completed successfully
2024-12-30 20:34:43,349 [INFO] Quantization successful
2024-12-30 20:34:43,349 [INFO] Starting training...
2024-12-30 20:46:26,257 [INFO] Initial device: cpu
2024-12-30 20:46:26,257 [INFO] Loading dataset...
2024-12-30 20:46:26,397 [INFO] Loading facebook/blenderbot-400M-distill with 4-bit quantization...
2024-12-30 20:46:27,560 [ERROR] Training failed: BlenderbotForConditionalGeneration does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 198, in train_quantized
    model = AutoModelForSeq2SeqLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 1225, in from_pretrained
    return super(BlenderbotForConditionalGeneration, cls).from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4173, in from_pretrained
    no_split_modules = model._get_no_split_modules(device_map)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 2072, in _get_no_split_modules
    raise ValueError(
ValueError: BlenderbotForConditionalGeneration does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.
2024-12-30 20:46:27,761 [ERROR] Script failed: BlenderbotForConditionalGeneration does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 356, in <module>
    train_quantized()
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 198, in train_quantized
    model = AutoModelForSeq2SeqLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 1225, in from_pretrained
    return super(BlenderbotForConditionalGeneration, cls).from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4173, in from_pretrained
    no_split_modules = model._get_no_split_modules(device_map)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 2072, in _get_no_split_modules
    raise ValueError(
ValueError: BlenderbotForConditionalGeneration does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.
2024-12-30 20:46:49,170 [INFO] Initial device: cpu
2024-12-30 20:46:49,170 [INFO] Loading dataset...
2024-12-30 20:46:49,282 [INFO] Loading facebook/blenderbot-400M-distill with 4-bit quantization...
2024-12-30 20:46:50,136 [ERROR] Training failed: BlenderbotForConditionalGeneration does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 198, in train_quantized
    model = AutoModelForSeq2SeqLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 1225, in from_pretrained
    return super(BlenderbotForConditionalGeneration, cls).from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4173, in from_pretrained
    no_split_modules = model._get_no_split_modules(device_map)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 2072, in _get_no_split_modules
    raise ValueError(
ValueError: BlenderbotForConditionalGeneration does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.
2024-12-30 20:46:50,291 [ERROR] Script failed: BlenderbotForConditionalGeneration does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 356, in <module>
    train_quantized()
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 198, in train_quantized
    model = AutoModelForSeq2SeqLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 1225, in from_pretrained
    return super(BlenderbotForConditionalGeneration, cls).from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4173, in from_pretrained
    no_split_modules = model._get_no_split_modules(device_map)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 2072, in _get_no_split_modules
    raise ValueError(
ValueError: BlenderbotForConditionalGeneration does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.
2024-12-30 20:47:29,386 [INFO] Initial device: cpu
2024-12-30 20:47:29,386 [INFO] Loading dataset...
2024-12-30 20:47:29,527 [INFO] Loading facebook/blenderbot-400M-distill with 4-bit quantization...
2024-12-30 20:47:30,528 [ERROR] Training failed: BlenderbotForConditionalGeneration does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 198, in train_quantized
    model = AutoModelForSeq2SeqLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 1225, in from_pretrained
    return super(BlenderbotForConditionalGeneration, cls).from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4173, in from_pretrained
    no_split_modules = model._get_no_split_modules(device_map)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 2072, in _get_no_split_modules
    raise ValueError(
ValueError: BlenderbotForConditionalGeneration does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.
2024-12-30 20:47:30,713 [ERROR] Script failed: BlenderbotForConditionalGeneration does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 356, in <module>
    train_quantized()
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 198, in train_quantized
    model = AutoModelForSeq2SeqLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 1225, in from_pretrained
    return super(BlenderbotForConditionalGeneration, cls).from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4173, in from_pretrained
    no_split_modules = model._get_no_split_modules(device_map)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 2072, in _get_no_split_modules
    raise ValueError(
ValueError: BlenderbotForConditionalGeneration does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.
2024-12-30 20:50:48,021 [INFO] Initial device: cpu
2024-12-30 20:50:48,021 [INFO] Loading dataset...
2024-12-30 20:50:48,148 [INFO] Loading facebook/blenderbot-400M-distill with 4-bit quantization...
2024-12-30 20:50:51,169 [INFO] Processing datasets...
2024-12-30 20:50:51,422 [INFO] Using default tokenizer.
2024-12-30 20:50:57,710 [INFO] Starting QLoRA training...
2024-12-30 20:51:00,094 [ERROR] Training failed: QuantizedLoRATrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 332, in train_quantized
    trainer.train()
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2524, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 3654, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: QuantizedLoRATrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
2024-12-30 20:51:00,238 [ERROR] Script failed: QuantizedLoRATrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 372, in <module>
    train_quantized()
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 332, in train_quantized
    trainer.train()
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2524, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 3654, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: QuantizedLoRATrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
2024-12-30 20:51:36,889 [INFO] Initial device: cpu
2024-12-30 20:51:36,889 [INFO] Loading dataset...
2024-12-30 20:51:37,008 [INFO] Loading facebook/blenderbot-400M-distill with 4-bit quantization...
2024-12-30 20:51:39,374 [INFO] Processing datasets...
2024-12-30 20:51:39,597 [INFO] Using default tokenizer.
2024-12-30 20:51:44,942 [INFO] Starting QLoRA training...
2024-12-30 20:51:47,523 [ERROR] Training failed: QuantizedLoRATrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 332, in train_quantized
    trainer.train()
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2524, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 3654, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: QuantizedLoRATrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
2024-12-30 20:51:47,672 [ERROR] Script failed: QuantizedLoRATrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 372, in <module>
    train_quantized()
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 332, in train_quantized
    trainer.train()
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2524, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 3654, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: QuantizedLoRATrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
2024-12-30 20:53:49,142 [INFO] Initial device: cpu
2024-12-30 20:53:49,142 [INFO] Loading dataset...
2024-12-30 20:53:49,264 [INFO] Loading facebook/blenderbot-400M-distill with 4-bit quantization...
2024-12-30 20:53:51,713 [INFO] Processing datasets...
2024-12-30 20:53:51,929 [INFO] Using default tokenizer.
2024-12-30 20:53:56,715 [INFO] Starting QLoRA training...
2024-12-30 20:53:59,137 [ERROR] Training failed: QuantizedLoRATrainer.training_step() takes 3 positional arguments but 4 were given
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 347, in train_quantized
    trainer.train()
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2524, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: QuantizedLoRATrainer.training_step() takes 3 positional arguments but 4 were given
2024-12-30 20:53:59,277 [ERROR] Script failed: QuantizedLoRATrainer.training_step() takes 3 positional arguments but 4 were given
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 387, in <module>
    train_quantized()
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 347, in train_quantized
    trainer.train()
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2524, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: QuantizedLoRATrainer.training_step() takes 3 positional arguments but 4 were given
2024-12-30 21:01:36,914 [INFO] Initial device: cpu
2024-12-30 21:01:36,914 [INFO] Loading dataset...
2024-12-30 21:01:37,043 [INFO] Loading facebook/blenderbot-400M-distill with 4-bit quantization...
2024-12-30 21:01:41,809 [INFO] Processing datasets...
2024-12-30 21:01:42,039 [INFO] Using default tokenizer.
2024-12-30 21:01:46,898 [INFO] Starting QLoRA training...
2024-12-30 21:01:49,164 [ERROR] Training failed: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 351, in train_quantized
    trainer.train()
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2524, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 153, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 132, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/accelerate/utils/operations.py", line 823, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/accelerate/utils/operations.py", line 811, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/peft/peft_model.py", line 1994, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 197, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 1296, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 1149, in forward
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 731, in forward
    inputs_embeds = self.embed_tokens(input_ids)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
    return inner()
           ^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 101, in forward
    return super().forward(input_ids) * self.embed_scale
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/sparse.py", line 190, in forward
    return F.embedding(
           ^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/functional.py", line 2551, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
2024-12-30 21:01:49,308 [ERROR] Script failed: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 391, in <module>
    train_quantized()
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 351, in train_quantized
    trainer.train()
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2524, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 153, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 132, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/accelerate/utils/operations.py", line 823, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/accelerate/utils/operations.py", line 811, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/peft/peft_model.py", line 1994, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 197, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 1296, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 1149, in forward
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 731, in forward
    inputs_embeds = self.embed_tokens(input_ids)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
    return inner()
           ^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/models/blenderbot/modeling_blenderbot.py", line 101, in forward
    return super().forward(input_ids) * self.embed_scale
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/modules/sparse.py", line 190, in forward
    return F.embedding(
           ^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/torch/nn/functional.py", line 2551, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
2024-12-30 21:03:20,994 [INFO] Initial device: cuda
2024-12-30 21:03:20,994 [INFO] Loading dataset...
2024-12-30 21:03:21,117 [INFO] Loading facebook/blenderbot-400M-distill with 4-bit quantization...
2024-12-30 21:03:23,949 [INFO] Processing datasets...
2024-12-30 21:03:24,165 [INFO] Using default tokenizer.
2024-12-30 21:03:29,761 [INFO] Starting QLoRA training...
2024-12-30 21:03:32,984 [ERROR] Training failed: 'QuantizedLoRATrainer' object has no attribute 'scaler'
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 351, in train_quantized
    trainer.train()
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2524, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 159, in training_step
    self.scaler.scale(loss).backward()
    ^^^^^^^^^^^
AttributeError: 'QuantizedLoRATrainer' object has no attribute 'scaler'
2024-12-30 21:03:33,154 [ERROR] Script failed: 'QuantizedLoRATrainer' object has no attribute 'scaler'
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 391, in <module>
    train_quantized()
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 351, in train_quantized
    trainer.train()
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2524, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 159, in training_step
    self.scaler.scale(loss).backward()
    ^^^^^^^^^^^
AttributeError: 'QuantizedLoRATrainer' object has no attribute 'scaler'
2024-12-30 21:06:28,839 [INFO] Using device: cuda
2024-12-30 21:06:28,839 [INFO] Loading dataset...
2024-12-30 21:06:31,290 [INFO] Processing datasets...
2024-12-30 21:06:31,513 [INFO] Using default tokenizer.
2024-12-30 21:06:36,070 [INFO] Starting QLoRA training...
2024-12-30 21:06:38,231 [ERROR] Training failed: QuantizedLoRATrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 309, in train_quantized
    trainer.train()
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2524, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 3654, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: QuantizedLoRATrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
2024-12-30 21:06:38,372 [ERROR] Script failed: QuantizedLoRATrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 349, in <module>
    train_quantized()
  File "/home/zahemen/projects/dl-lib/DocAnalyzerAI/src/train_finbot_quantized.py", line 309, in train_quantized
    trainer.train()
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 2524, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_env/lib/python3.11/site-packages/transformers/trainer.py", line 3654, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: QuantizedLoRATrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
